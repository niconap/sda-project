{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network\n",
    "To predict whether or not a patient has Alzheimer's disease, a neural network is used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from data_reader import get_data_dict, split_data\n",
    "data = get_data_dict('./data/alzheimers_disease_data.csv')\n",
    "metadata, internal_factors, external_factors = split_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Values used\n",
    "From our original exploration of the data, we found that there were six variables that showed some sort of relationship to the diagnosis of a patient on their own:\n",
    "1. `Hypertension`\n",
    "2. `MemoryComplaints`\n",
    "3. `BehavioralProblems`\n",
    "4. `MMSE`\n",
    "5. `FunctionalAssessment`\n",
    "6. `ADL`\n",
    "\n",
    "For creating the neural network, our data set will be split into a training set and a test set. The test set will contain the last 500 rows of the set, so there are 1650 datapoints left for the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Age', 'Hypertension', 'MemoryComplaints', 'BehavioralProblems', 'MMSE',\n",
    "           'FunctionalAssessment', 'ADL']\n",
    "test_set_size = 500\n",
    "learning_set_size = len(data['PatientID']) - test_set_size\n",
    "\n",
    "feature_matrix = np.vstack(tuple(data[col][:learning_set_size] for col in columns)).T\n",
    "target = np.array(data['Diagnosis'][:learning_set_size])\n",
    "test_matrix = np.vstack(tuple(data[col][learning_set_size:] for col in columns)).T\n",
    "test_target = np.array(data['Diagnosis'][learning_set_size:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure of the NN\n",
    "Deciding the structure of an NN is more of an art than a science (_Rein van den Boogaard_), so to start with we use three layers, starting with vectors of size 6, then 6, then 3 which reduces to 1 to end up at one final value. The feature vectors are of size 6, so the weight matrices should match that size. For the activation function, the sigmoid function is used. Using the ReLU function does not do much here, as the values in the data are not extremely small or big."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN():\n",
    "    def __init__(self, layerSizes=[7,6,5,5,3,2]):\n",
    "        self.layers = layerSizes + [1]\n",
    "        self.n_layers = len(layerSizes)\n",
    "\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "\n",
    "        # First, the weights and biases should be randomly initialized\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            # Since sigmoid is used as an activation function, Xavier init is\n",
    "            # used here\n",
    "            limit = np.sqrt(6 / sum((self.layers[i], self.layers[i + 1])))\n",
    "            W = np.random.uniform(-limit, limit, size=(self.layers[i + 1], self.layers[i]))\n",
    "            b = np.random.uniform(-limit, limit, size=(self.layers[i + 1],))\n",
    "            self.weights.append(W)\n",
    "            self.biases.append(b)\n",
    "\n",
    "    def loss(self, y_hat, y_true):\n",
    "        # Since we need a binary outcome, we use binary cross-entropy loss\n",
    "        y_hat = np.clip(y_hat, 1e-6, 1 - 1e-6)\n",
    "        return -np.mean(y_true * np.log(y_hat) + (1 - y_true) * np.log(1 - y_hat))\n",
    "\n",
    "    def activate(self, v):\n",
    "        # The activation function (sigmoid)\n",
    "        return 1/(1 + np.exp(-v))\n",
    "\n",
    "    def d_activate(self, v):\n",
    "        # Derivative of the activation function\n",
    "        return self.activate(v) * (1 - self.activate(v))\n",
    "\n",
    "    def predict(self, x):\n",
    "        # Given a feature vector x, predict the target value using the network\n",
    "        self.z = []\n",
    "        self.a = [x]\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            z = x @ self.weights[i].T + self.biases[i]\n",
    "            self.z.append(z)\n",
    "            x = self.activate(z)\n",
    "            self.a.append(x)\n",
    "        return x\n",
    "\n",
    "    def back_propagation(self, y):\n",
    "        # Update the weight values according to an outcome a\n",
    "        pass\n",
    "\n",
    "    def train(self, x, y, alpha=0.1, batch_size=32):\n",
    "        # Floor divide to see the last iteration (which might be smaller than\n",
    "        # the batch size) a special case\n",
    "        for i in range(len(x)//batch_size):\n",
    "            derivs = [0] * self.n_layers\n",
    "            prediction = self.predict(x[batch_size*i:batch_size*i + batch_size])\n",
    "            # Perform a back propagation based on the loss\n",
    "            l = self.loss(prediction, y)\n",
    "            derivs[-1] = l\n",
    "            for j in range(len(derivs) - 2, 0, -1):\n",
    "                derivs[j] = (self.d_activate(self.z[j])*derivs[j + 1]) @ self.weights[j]\n",
    "            for j in range(len(self.layers) - 2):\n",
    "                self.biases[j] -= alpha * np.sum(self.d_activate(self.z[j])*derivs[j + 1])\n",
    "                self.weights[j] -= alpha * (self.d_activate(self.z[j])*derivs[j + 1]).T @ self.a[j]\n",
    "\n",
    "\n",
    "nn = NN()\n",
    "prediction = nn.predict(test_matrix[:10])\n",
    "rounded = np.where(prediction < 0.5, 0, 1)\n",
    "print(rounded.T[0])\n",
    "print(test_target[:10])\n",
    "nn.train(feature_matrix, target)\n",
    "prediction = nn.predict(test_matrix[:10])\n",
    "rounded = np.where(prediction < 0.5, 0, 1)\n",
    "print(rounded.T[0])\n",
    "print(test_target[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_size = 500\n",
    "learning_set_size = len(data['PatientID']) - test_set_size\n",
    "\n",
    "columns = list(data.keys())\n",
    "columns.remove('DoctorInCharge')\n",
    "columns.remove('Diagnosis')\n",
    "\n",
    "feature_matrix = np.vstack(tuple(data[col][:learning_set_size] for col in columns)).T\n",
    "target = np.array(data['Diagnosis'][:learning_set_size])\n",
    "test_matrix = np.vstack(tuple(data[col][learning_set_size:] for col in columns)).T\n",
    "test_target = np.array(data['Diagnosis'][learning_set_size:])\n",
    "\n",
    "nn = NN(layerSizes=[33, 16, 8, 4, 2])\n",
    "prediction = nn.predict(test_matrix[:10])\n",
    "rounded = np.where(prediction < 0.5, 0, 1)\n",
    "print(rounded.T[0])\n",
    "print(test_target[:10])\n",
    "nn.train(feature_matrix, target)\n",
    "prediction = nn.predict(test_matrix[:10])\n",
    "rounded = np.where(prediction < 0.5, 0, 1)\n",
    "print(rounded.T[0])\n",
    "print(test_target[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sadly, the neural network did not produce good results. Even other implementations, that definitely work, did not yield good predictions at all. Even when they were tested with the full feature vector instead of just the six fields mentioned above. Therefore, we decided to scrap this idea and instead use a different data set to test the accuracy our regression model with."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
